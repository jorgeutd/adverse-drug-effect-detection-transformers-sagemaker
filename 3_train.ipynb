{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e605aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c49d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade\n",
    "#!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bc48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683a661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb5965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::197614225699:role/bi-sagemaker-access\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "# print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee09503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8faf86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f's3://{bucket}/processing_output/train_data'\n",
    "val_input_path = f's3://{bucket}/processing_output/validation_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf103cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://az-ade-197614225699/processing_output/validation_data'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b98af0",
   "metadata": {},
   "source": [
    "### Set up Huggingface training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1050c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 10,\n",
    "                 'train_batch_size': 16,\n",
    "                 'model_name':'distilbert-base-uncased',\n",
    "                 'do_eval': True,\n",
    "                 'load_best_model_at_end':True\n",
    "                 }\n",
    "\n",
    "#configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True}}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a69e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instance configurations\n",
    "# instance_type='ml.p3dn.24xlarge'\n",
    "# instance_count=1\n",
    "# volume_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc457b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size=1000,\n",
    "    role=role,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    output_path=f's3://{bucket}/training_output/',\n",
    "    base_job_name=\"az-ade-training\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,\n",
    "    distribution=distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86da7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-18 18:46:36 Starting - Starting the training job...\n",
      "2021-11-18 18:46:48 Starting - Launching requested ML instances.........\n",
      "2021-11-18 18:48:36 Starting - Preparing the instances for training.........\n",
      "2021-11-18 18:50:01 Downloading - Downloading input data...\n",
      "2021-11-18 18:50:28 Training - Downloading the training image.................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:16,843 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:16,921 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:19,947 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:19,948 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,433 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,433 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,436 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,436 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,436 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,440 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-11-18 18:53:20,517 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"load_best_model_at_end\": true,\n",
      "        \"train_batch_size\": 16,\n",
      "        \"do_eval\": true,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"az-ade-training-2021-11-18-18-46-36-028\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://az-ade-197614225699/az-ade-training-2021-11-18-18-46-36-028/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"epochs\":10,\"load_best_model_at_end\":true,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://az-ade-197614225699/az-ade-training-2021-11-18-18-46-36-028/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"epochs\":10,\"load_best_model_at_end\":true,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"az-ade-training-2021-11-18-18-46-36-028\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://az-ade-197614225699/az-ade-training-2021-11-18-18-46-36-028/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--epochs\",\"10\",\"--load_best_model_at_end\",\"True\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so smddprun /opt/conda/bin/python3.6 -m mpi4py train.py --do_eval True --epochs 10 --load_best_model_at_end True --model_name distilbert-base-uncased --train_batch_size 16\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:['/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/lib', '/opt/ml/code', '/opt/ml/code', '/opt/conda/bin', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/opt/conda/lib/python3.6/site-packages']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-11-18 18:53:16 Training - Training image download completed. Training in progress.\u001b[34m[1,7]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:26,248 - __main__ - INFO -  loaded train_dataset length is: 14627\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:26,249 - __main__ - INFO -  loaded val_dataset length is: 3134\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:26,282 - filelock - INFO - Lock 140023537168168 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:26,309 - filelock - INFO - Lock 140023537168168 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:26,332 - filelock - INFO - Lock 139816974055408 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:26,332 - filelock - INFO - Lock 139816974055408 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:26,333 - filelock - INFO - Lock 140403144046464 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:26,333 - filelock - INFO - Lock 140403144046464 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:26,335 - filelock - INFO - Lock 140324798144296 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:26,335 - filelock - INFO - Lock 140324798144296 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:26,337 - filelock - INFO - Lock 139908552625960 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:26,337 - filelock - INFO - Lock 139908552625960 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:26,338 - filelock - INFO - Lock 139938112470824 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:26,338 - filelock - INFO - Lock 139938112470824 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:26,339 - filelock - INFO - Lock 140023537168168 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:26,339 - filelock - INFO - Lock 140298580803368 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:26,340 - filelock - INFO - Lock 140298580803368 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:26,382 - filelock - INFO - Lock 139780488245032 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:26,383 - filelock - INFO - Lock 139780488245032 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:31,301 - filelock - INFO - Lock 140023537168168 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:31,329 - filelock - INFO - Lock 139780357412456 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:31,330 - filelock - INFO - Lock 139780357412456 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:31,332 - filelock - INFO - Lock 140403144046184 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:31,333 - filelock - INFO - Lock 140403144046184 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:31,333 - filelock - INFO - Lock 140324440790632 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:31,334 - filelock - INFO - Lock 140324440790632 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:31,335 - filelock - INFO - Lock 139907831990144 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:31,336 - filelock - INFO - Lock 139907831990144 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:31,342 - filelock - INFO - Lock 140298450052992 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:31,343 - filelock - INFO - Lock 140298450052992 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:31,380 - filelock - INFO - Lock 139816968075192 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:31,381 - filelock - INFO - Lock 139816968075192 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:31,385 - filelock - INFO - Lock 139938466854728 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:31,386 - filelock - INFO - Lock 139938466854728 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:32,172 - filelock - INFO - Lock 139907832073632 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:32,205 - filelock - INFO - Lock 139907832073632 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:32,228 - filelock - INFO - Lock 140022823972144 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:32,228 - filelock - INFO - Lock 140022823972144 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:32,233 - filelock - INFO - Lock 139907832071280 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:32,234 - filelock - INFO - Lock 139780357492632 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:32,235 - filelock - INFO - Lock 139780357492632 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:32,235 - filelock - INFO - Lock 140324440870640 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:32,235 - filelock - INFO - Lock 140324440870640 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:32,238 - filelock - INFO - Lock 140298450136480 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:32,238 - filelock - INFO - Lock 140298450136480 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:32,239 - filelock - INFO - Lock 140403144126024 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:32,239 - filelock - INFO - Lock 140403144126024 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:32,275 - filelock - INFO - Lock 139907832071280 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:32,291 - filelock - INFO - Lock 139816968155320 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:32,291 - filelock - INFO - Lock 139816968155320 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:32,299 - filelock - INFO - Lock 139938112508368 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:32,299 - filelock - INFO - Lock 139938112508368 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:32,310 - filelock - INFO - Lock 140022823972704 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:2021-11-18 18:53:32,310 - filelock - INFO - Lock 140022823972704 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:32,313 - filelock - INFO - Lock 140324440868400 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:2021-11-18 18:53:32,314 - filelock - INFO - Lock 140324440868400 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:32,316 - filelock - INFO - Lock 140298450134128 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:2021-11-18 18:53:32,316 - filelock - INFO - Lock 140298450134128 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:32,317 - filelock - INFO - Lock 139780357492464 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:2021-11-18 18:53:32,317 - filelock - INFO - Lock 139780357492464 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:32,318 - filelock - INFO - Lock 140403144122552 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:2021-11-18 18:53:32,318 - filelock - INFO - Lock 140403144122552 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:32,358 - filelock - INFO - Lock 139907832071280 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:2021-11-18 18:53:32,382 - filelock - INFO - Lock 139907832071280 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:32,422 - filelock - INFO - Lock 139816968156832 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:2021-11-18 18:53:32,423 - filelock - INFO - Lock 139816968156832 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:32,432 - filelock - INFO - Lock 139938112508032 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:2021-11-18 18:53:32,433 - filelock - INFO - Lock 139938112508032 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Bootstrap : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO NET/Socket : Using [0]eth0:10.2.218.164<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO comm 0x5623b88c34f0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO comm 0x55993c72bcd0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO comm 0x5605a6bacc80 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO comm 0x562bdaa03b80 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO comm 0x55983acc9710 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO comm 0x55b9b50201a0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO comm 0x55c4c1b00b00 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO comm 0x5563be986630 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:38:38 [1] NCCL INFO comm 0x5623bb5caca0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:36 [0] NCCL INFO comm 0x55993f433480 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:42:42 [3] NCCL INFO comm 0x5605a98b4430 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:40:40 [2] NCCL INFO comm 0x562bdd70b330 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:44:44 [4] NCCL INFO comm 0x55983d9d0ec0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:46:46 [5] NCCL INFO comm 0x55b9b7d27950 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:47:47 [6] NCCL INFO comm 0x5563c168dde0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:48:48 [7] NCCL INFO comm 0x55c4c48082b0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:36:758 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:40.843 algo-1:46 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:40.843 algo-1:47 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:40.843 algo-1:38 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:40.843 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:40.843 algo-1:40 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:40.848 algo-1:48 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:40.849 algo-1:44 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:40.851 algo-1:36 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:40.933 algo-1:42 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:40.933 algo-1:46 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:40.933 algo-1:44 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:40.933 algo-1:48 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:40.933 algo-1:47 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:40.933 algo-1:40 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:40.933 algo-1:47 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:40.933 algo-1:44 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:40.933 algo-1:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:40.934 algo-1:46 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:40.934 algo-1:48 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:40.934 algo-1:47 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:40.934 algo-1:40 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:40.934 algo-1:38 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:40.934 algo-1:42 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:40.934 algo-1:44 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:40.934 algo-1:46 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:40.934 algo-1:48 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:40.934 algo-1:36 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:40.934 algo-1:40 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:40.935 algo-1:42 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:40.935 algo-1:46 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:40.935 algo-1:47 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:40.935 algo-1:38 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:40.935 algo-1:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:40.935 algo-1:36 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:40.935 algo-1:47 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:40.935 algo-1:44 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:40.935 algo-1:46 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:40.935 algo-1:48 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:40.935 algo-1:44 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:40.935 algo-1:48 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:40.936 algo-1:36 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:40.936 algo-1:40 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:40.936 algo-1:40 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:40.936 algo-1:38 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:40.937 algo-1:36 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:40.938 algo-1:36 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:40.938 algo-1:38 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:40.938 algo-1:38 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.123 algo-1:44 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.123 algo-1:44 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.124 algo-1:44 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.124 algo-1:44 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.124 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.124 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.124 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.125 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.125 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.125 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.125 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.125 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.125 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.126 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.126 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.126 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.126 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.126 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.127 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.128 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.129 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.130 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.131 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.132 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.133 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.134 algo-1:42 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.134 algo-1:42 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.134 algo-1:42 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.134 algo-1:42 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.134 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.134 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.135 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.135 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.135 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.135 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.135 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.135 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.135 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.135 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.135 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.135 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.135 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.136 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.137 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.138 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.139 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.140 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.140 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.140 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.140 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.141 algo-1:44 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.141 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-18 18:53:41.142 algo-1:44 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.142 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.142 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.142 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.142 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.142 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.143 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.144 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.144 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.144 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.144 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.144 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.144 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.145 algo-1:42 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.146 algo-1:42 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.146 algo-1:42 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.146 algo-1:42 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-18 18:53:41.146 algo-1:42 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.156 algo-1:38 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.157 algo-1:38 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.157 algo-1:38 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.157 algo-1:38 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.157 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.157 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.157 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.158 algo-1:46 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.158 algo-1:46 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.158 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.159 algo-1:46 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.159 algo-1:46 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.159 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.159 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.159 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.160 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.160 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.160 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.160 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.160 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.160 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.160 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.160 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.160 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.160 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.160 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.160 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.161 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.161 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.161 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.161 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.161 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.161 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.161 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.161 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.161 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.161 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.161 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.162 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.162 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.162 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.162 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.162 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.162 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.162 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.162 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.162 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.162 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.163 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.163 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.163 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.163 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.163 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.163 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.163 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.163 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.163 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.163 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.163 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.164 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.164 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.164 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.164 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.164 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.164 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.164 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.164 algo-1:48 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.164 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.164 algo-1:48 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.164 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.164 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.165 algo-1:48 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.165 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.165 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.165 algo-1:48 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.165 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.165 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.165 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.165 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.165 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.165 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.165 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.165 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.165 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.165 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.166 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.166 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.166 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.166 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.166 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.166 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.166 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.166 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.166 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.166 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.166 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.166 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.166 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.166 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.166 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.166 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.167 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.167 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.167 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.167 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.167 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.167 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.167 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.167 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.167 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.167 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.167 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.167 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.167 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.167 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.168 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.168 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.168 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.168 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.168 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.168 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.168 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.168 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.168 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.168 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.168 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.168 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.168 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.169 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.169 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.169 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.169 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.169 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.169 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.169 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.169 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.169 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.169 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.169 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.169 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.169 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.169 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.170 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.170 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.170 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.170 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.170 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.170 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.170 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.170 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.170 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.170 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.170 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.170 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.170 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.170 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.170 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.170 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.171 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.171 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.171 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.171 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.171 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.171 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.171 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.171 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.171 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.171 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.171 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.171 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.171 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.171 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.172 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.172 algo-1:40 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.172 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.172 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.172 algo-1:40 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.172 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.172 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.172 algo-1:40 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.172 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.172 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.172 algo-1:40 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.172 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.172 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.172 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.173 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.173 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.173 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.173 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.173 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.173 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.173 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.173 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.173 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.173 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.173 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.173 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.173 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.173 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.173 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.174 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.174 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.174 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.174 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.174 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.174 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.174 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.174 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.174 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.174 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.174 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.174 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.174 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.174 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.174 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.174 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.175 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.175 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.175 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.175 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.175 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.175 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.175 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.175 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.175 algo-1:38 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.175 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.176 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.176 algo-1:38 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.176 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.176 algo-1:38 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.176 algo-1:38 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.176 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.176 algo-1:38 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.176 algo-1:38 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.176 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-18 18:53:41.176 algo-1:38 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.176 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.176 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.176 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.177 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.177 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.177 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.177 algo-1:47 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.177 algo-1:46 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.177 algo-1:47 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.177 algo-1:48 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.177 algo-1:47 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.178 algo-1:48 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.177 algo-1:46 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.177 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.178 algo-1:48 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.178 algo-1:48 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.178 algo-1:46 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.178 algo-1:48 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.178 algo-1:48 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.178 algo-1:46 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-18 18:53:41.178 algo-1:48 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.178 algo-1:46 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.178 algo-1:46 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.178 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.178 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-18 18:53:41.179 algo-1:46 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.179 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.179 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.179 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.179 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.179 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.179 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.179 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.180 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.180 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.180 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.180 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.180 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.180 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.180 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.181 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.181 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.182 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.182 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.183 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.183 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.183 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.183 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.183 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.183 algo-1:40 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.184 algo-1:40 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-18 18:53:41.184 algo-1:40 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.184 algo-1:36 INFO hook.py:591] name:module.distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.184 algo-1:36 INFO hook.py:591] name:module.distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.184 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.184 algo-1:36 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.185 algo-1:36 INFO hook.py:591] name:module.distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.185 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.185 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.185 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.185 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.185 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.186 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.186 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.186 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.186 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.186 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.186 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.186 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.186 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.186 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.186 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.187 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.187 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.187 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.187 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.187 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.187 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.187 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.188 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.188 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.188 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.188 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.188 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.189 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.189 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.190 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.190 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.190 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.190 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.190 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.190 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.191 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.191 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.191 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.191 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.191 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.191 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.191 algo-1:47 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.192 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-18 18:53:41.192 algo-1:47 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.192 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.192 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.192 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.192 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.192 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.193 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.193 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.193 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.193 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.193 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.193 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.194 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.195 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.195 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.195 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.195 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.195 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.195 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.196 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.196 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.196 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.196 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.196 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.197 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.197 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.197 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.197 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.197 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.198 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.198 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.198 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.198 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.198 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.198 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.199 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.200 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.200 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.200 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.200 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.200 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.200 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.201 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.201 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.201 algo-1:36 INFO hook.py:591] name:module.distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.201 algo-1:36 INFO hook.py:591] name:module.pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.201 algo-1:36 INFO hook.py:591] name:module.pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.202 algo-1:36 INFO hook.py:591] name:module.classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.202 algo-1:36 INFO hook.py:591] name:module.classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.202 algo-1:36 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-18 18:53:41.202 algo-1:36 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stdout>:{'eval_loss': 0.28335246443748474, 'eval_accuracy': 0.8857689853222719, 'eval_f1': 0.7055921052631579, 'eval_precision': 0.7447916666666666, 'eval_recall': 0.6703125, 'eval_runtime': 2.5723, 'eval_samples_per_second': 1218.387, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'eval_loss': 0.1827087551355362, 'eval_accuracy': 0.923739629865986, 'eval_f1': 0.798990748528175, 'eval_precision': 0.8652094717668488, 'eval_recall': 0.7421875, 'eval_runtime': 2.5453, 'eval_samples_per_second': 1231.274, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'eval_loss': 0.16968166828155518, 'eval_accuracy': 0.9320357370772177, 'eval_f1': 0.8418708240534521, 'eval_precision': 0.801980198019802, 'eval_recall': 0.8859375, 'eval_runtime': 2.5507, 'eval_samples_per_second': 1228.661, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'eval_loss': 0.2614574730396271, 'eval_accuracy': 0.9042756860242501, 'eval_f1': 0.8007968127490039, 'eval_precision': 0.6963048498845266, 'eval_recall': 0.9421875, 'eval_runtime': 2.5181, 'eval_samples_per_second': 1244.568, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'loss': 0.2423, 'learning_rate': 5e-05, 'epoch': 4.35}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'eval_loss': 0.21033325791358948, 'eval_accuracy': 0.9342693044033185, 'eval_f1': 0.8487518355359766, 'eval_precision': 0.8005540166204986, 'eval_recall': 0.903125, 'eval_runtime': 2.5511, 'eval_samples_per_second': 1228.513, 'epoch': 5.0}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'val': val_input_path})\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48cebf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://az-ade-197614225699/training_output/az-ade-training-2021-11-18-18-46-36-028/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(huggingface_estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77ad69",
   "metadata": {},
   "source": [
    "### Save training job name for next session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb2bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'az-ade-training-2021-11-18-18-46-36-028'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_job_name = huggingface_estimator.latest_training_job.name\n",
    "training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "859eefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2864e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Upload the fine-tuned model to huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb9d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface\n",
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a841dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.63.2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "361a22ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::197614225699:role/bi-sagemaker-access\n",
      "sagemaker bucket: sagemaker-us-east-1-197614225699\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "#sagemaker_session_bucket=\"samsum-dataset\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52167e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'my_destilbert_model_ADEs'\n",
    "\n",
    "os.makedirs(local_path, exist_ok = True)\n",
    "\n",
    "# download model from S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f\"{local_path}/model.tar.gz\", \"r:gz\")\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f\"{local_path}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e60d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import os\n",
    "import math\n",
    "import warnings \n",
    "from datetime import datetime, date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf66519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read eval and test results \n",
    "# with open(f\"{local_path}/eval_results.json\") as f:\n",
    "#     eval_results_raw = json.load(f)\n",
    "#     print(eval_results_raw)\n",
    "#     df_results = pd.json_normalize(eval_results_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789492ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
